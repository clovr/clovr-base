#!/usr/bin/env bash

################################################################################
# Script that is run on each EC2 instance on boot. It is passed in the EC2 user
# data, so should not exceed 16K in size.
################################################################################

################################################################################
# Initialize variables
################################################################################

# Slaves are started after the master, and are told its address by sending a
# modified copy of this file which sets the MASTER_HOST variable. 
# A node  knows if it is the master or not by inspecting the security group
# name. If it is the master then it retrieves its address using instance data.
MASTER_HOST=%MASTER_HOST% # Interpolated before being sent to EC2 node
SECURITY_GROUPS=`wget -q -O - http://169.254.169.254/latest/meta-data/security-groups`
IS_MASTER=`echo $SECURITY_GROUPS | awk '{ a = match ($0, "-master$"); if (a) print "true"; else print "false"; }'`
if [ "$IS_MASTER" == "true" ]; then
 MASTER_HOST=`wget -q -O - http://169.254.169.254/latest/meta-data/local-hostname`
fi

WORKER_FILES=%WORKER_FILES%

HADOOP_HOME=`ls -d /usr/local/hadoop-*`

################################################################################
# Hadoop configuration
# Modify this section to customize your Hadoop cluster.
################################################################################

cat > $HADOOP_HOME/conf/hadoop-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>hadoop.tmp.dir</name>
  <value>/mnt/hadoop</value>
</property>

<property>
  <name>fs.default.name</name>
  <value>hdfs://$MASTER_HOST:50001</value>
</property>

<property>
  <name>mapred.job.tracker</name>
  <value>hdfs://$MASTER_HOST:50002</value>
</property>

<property>
  <name>tasktracker.http.threads</name>
  <value>80</value>
</property>

<property>
  <name>mapred.tasktracker.map.tasks.maximum</name>
  <value>%MAP_MAX%</value>
</property>

<property>
  <name>mapred.tasktracker.reduce.tasks.maximum</name>
  <value>%REDUCE_MAX%</value>
</property>

<property>
  <name>mapred.output.compress</name>
  <value>false</value>
</property>

<property>
  <name>mapred.output.compression.type</name>
  <value>BLOCK</value>
</property>

<property>
  <name>dfs.client.block.write.retries</name>
  <value>3</value>
</property>

<property>
  <name>mapred.task.timeout</name>
  <value>6000000</value>
</property>

<property>
  <name>mapred.max.tracker.failures</name>
  <value>%FAIL_MAX%</value>
</property>

<!-- BTL: The following two jets3t properties are modified to avoid a
     socket read timeout exception that, we think, happens because
     Bowtie takes too long to analyze a given buffer of data.
     Originally they were both set to 1 minute.
     
     Update: I don't think this helped.  The real solution was to use
     bowtie_wrap.pl.
-->

<property>
  <name>httpclient.connection-timeout-ms</name>
  <value>600000</value>
</property>

<property>
  <name>httpclient.socket-timeout-ms</name>
  <value>600000</value>
</property>

<!-- BTL: Following property is to prevent the reducers from running at
     the same time as the mappers, which seemed to lead to systemwide
     deadlock.
     https://issues.apache.org/jira/browse/HADOOP-3136
-->

<property>
  <name>mapred.reduce.slowstart.completed.maps</name>
  <value>1.0</value>
</property>

</configuration>
EOF

# Configure Hadoop for Ganglia
# overwrite hadoop-metrics.properties
cat > $HADOOP_HOME/conf/hadoop-metrics.properties <<EOF

# Ganglia
# we push to the master gmond so hostnames show up properly
dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext
dfs.period=10
dfs.servers=$MASTER_HOST:8649

mapred.class=org.apache.hadoop.metrics.ganglia.GangliaContext
mapred.period=10
mapred.servers=$MASTER_HOST:8649

jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
jvm.period=10
jvm.servers=$MASTER_HOST:8649
EOF

################################################################################
# Start services
################################################################################

[ ! -f /etc/hosts ] &&  echo "127.0.0.1 localhost" > /etc/hosts

mkdir -p /mnt/hadoop/logs

# not set on boot
export USER="root"

if [ "$IS_MASTER" == "true" ]; then
  # MASTER
  # Prep Ganglia
  sed -i -e "s|\( *mcast_join *=.*\)|#\1|" \
         -e "s|\( *bind *=.*\)|#\1|" \
         -e "s|\( *mute *=.*\)|  mute = yes|" \
         -e "s|\( *location *=.*\)|  location = \"master-node\"|" \
         /etc/gmond.conf
  mkdir -p /mnt/ganglia/rrds
  chown -R ganglia:ganglia /mnt/ganglia/rrds
  rm -rf /var/lib/ganglia; cd /var/lib; ln -s /mnt/ganglia ganglia; cd
  service gmond start
  service gmetad start
  apachectl start

  # Hadoop
  # only format on first boot
  [ ! -e /mnt/hadoop/dfs ] && "$HADOOP_HOME"/bin/hadoop namenode -format

  "$HADOOP_HOME"/bin/hadoop-daemon.sh start namenode
  "$HADOOP_HOME"/bin/hadoop-daemon.sh start jobtracker
else
  # SLAVE
  # Prep Ganglia
  sed -i -e "s|\( *mcast_join *=.*\)|#\1|" \
         -e "s|\( *bind *=.*\)|#\1|" \
         -e "s|\(udp_send_channel {\)|\1\n  host=$MASTER_HOST|" \
         /etc/gmond.conf
  service gmond start
  
  pushd /mnt
  for f in $WORKER_FILES ; do
	# Parse out the MD5 if included
	md5=
	if echo $f | grep -q '::' ; then
		md5=`echo $f | sed -e 's/.*:://'`
		f=`echo $f | sed -e 's/::.*//'`
	fi
	fbn=`basename $f`
	rm -f /root/.${fbn}.md5err
	
	while true ; do
		# Get the file, perhaps streaming through 'jar x' if it's a jar
		success=1
		if echo $f | grep -q '\.jar$' ; then
			touch /root/.${fbn}.is_a_jar
			jar_exe=`find /usr/local -name jar`
			if [ -n "$jar_exe" ] ; then
				# Unjar on the fly while it's downloading, but also store
				# it to a file so that we can md5sum it later
				curl $f | tee $fbn | $jar_exe x
			else
				dt=`date`
				echo "$dt: Could not find a 'jar' executable" >> /root/.${fbn}.jarerr
				touch /root/.${fbn}.no_jar_exe
				exit 1
			fi
		else
			wget $f
		fi
		# Check that actual MD5 matches expectation
		if [ -n "$md5" ] ; then
			omd5=`md5sum $fbn | cut -d' ' -f 1`
			if [ "$md5" != "$omd5" ] ; then
				dt=`date`
				echo "$dt: Expected MD5 $md5 for $fbn, got $omd5" >> /root/.${fbn}.md5err
				success=0
				sleep 10
			fi
		fi
		if [ $success -eq 1 ] ; then
			break
		fi
	done
  touch /root/.$f
  done

  # Set max shared memory size high for Bowtie
  echo 3221225472 > /proc/sys/kernel/shmmax

  popd

  # Hadoop
  "$HADOOP_HOME"/bin/hadoop-daemon.sh start datanode
  "$HADOOP_HOME"/bin/hadoop-daemon.sh start tasktracker
fi

# Run this script on next boot
rm -f /var/ec2/ec2-run-user-data.*
