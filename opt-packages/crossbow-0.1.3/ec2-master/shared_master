#!/bin/bash

#
# Check that all needed environment variables are set.
#
check_env() {
	if [ -z "$AWS_ACCOUNT_ID" ] ; then
		echo "Error, AWS_ACCOUNT_ID environment variable must be set"
		usagedie
	fi
	if [ -z "$AWS_ACCESS_KEY_ID" ] ; then
		echo "Error, AWS_ACCESS_KEY_ID environment variable must be set"
		usagedie
	fi
	if [ -z "$AWS_SECRET_ACCESS_KEY" ] ; then
		echo "Error, AWS_SECRET_ACCESS_KEY environment variable must be set"
		usagedie
	fi
	echo "Using AWS account id: $AWS_ACCOUNT_ID"
}

add_s3_key() {
	tmp=$1
	if [ -z "$AWS_ACCESS_KEY_ID" ] ; then
		echo "Error, AWS_ACCESS_KEY_ID must be set before calling add_s3_key"
		usagedie
	fi
	if [ -z "$AWS_SECRET_ACCESS_KEY" ] ; then
		echo "Error, AWS_SECRET_ACCESS_KEY must be set before calling add_s3_key"
		usagedie
	fi
	if ! echo $1 | grep -q '[@]' ; then
		EC2KEY="${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}"
		tmp=`echo "$tmp" | sed -e "s|s3n://|s3n://${EC2KEY}@|"`
		tmp=`echo "$tmp" | sed -e "s|s3://|s3://${EC2KEY}@|"`
	fi
	echo $tmp
}

bulldoze() {
	# Deal with output dir
	echo -n "Checking whether to bulldoze $1 at: " ; date
	if ${hadoop_script} fs -stat $1 2>/dev/null ; then
		# The output dir already exists.  Either print a message and
		# abort or bulldoze it.
		if [ ${bulldoze_dirs} = "1" ] ; then
			${hadoop_script} fs -rmr $1
		else
			echo "The output directory $1 already exists; either remove it or run copy_master with the -b option."
			exit 1
		fi
	fi
}
