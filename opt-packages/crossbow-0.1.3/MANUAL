Crossbow: Parallel short read genotyping in the cloud
http://bowtie-bio.sf.net/crossbow

Crossbow is by Ben Langmead (1,2) and Michael C. Schatz (2)

Bowtie is by Ben Langmead (1, 2) and Cole Trapnell (2)

SOAPsnp is by Ruiqiang Li (3,4), Yingrui Li (3) Xiaodong Fang (3),
  Huanming Yang (3), Jian Wang (3), Karsten Kristiansen (3, 4), and
  Jun Wang (3,4)

(1) Department of Biostatistics
Johns Hopkins Bloomberg School of Public Health
615 North Wolfe Street, Baltimore, MD 21205, USA

(2) Center for Bioinformatics and Computational Biology
University of Maryland, College Park, MD 20742, USA

(3) Beijing Genomics Institute at Shenzhen
Shenzhen 518000, China

(4) Department of Biochemistry and Molecular Biology
University of Southern Denmark
Odense M DK-5230, Denmark

Crossbow Manual
===============

 What is Crossbow?
 ================

 Crossbow is a scalable, portable, and automatic Cloud Computing tool
 for finding SNPs in mammalian genomes from short read data.  Crossbow
 employs modified versions of Bowtie and SOAPsnp to perform the short
 read alignment and SNP calling respectively.

 CAUTION!
 ========

 Renting resources from Amazon EC2 and S3 costs money, regardless of
 whether your experiment ultimately suceeds or fails.  In some cases,
 Crossbow or its documentation may be partially to blame for a failed
 experiment.  While we are happy to accept bug reports, we do not
 accept responsibility for any financial damage caused by these errors.
 Crossbow is provided "as is" with no warranty, express or implied.
 See the CB_LICENSE and LICENSES files for more details about the
 licenses associated with the files in the Crossbow package.

 General Prerequisites
 =====================

 To run Crossbow on a local computer cluster, that cluster must have
 Hadoop installed.  Crossbow was tested using Hadoop 0.20.0.
 
 Workers in the Hadoop cluster must sufficient memory available.  The
 exact amount needed depends on the size of the genome being analyzed.
 For analysis of the human genome, it is recommended that cluster
 computers have at least 7 GB of memory available.  When running jobs
 using Amazon's EC2 service, the Extra-Large High-CPU node type is
 recommended.
 
 The computer from which the Crossbow scripts will be run must have
 basic UNIXy tools such as sh, bash, openssh, sed, grep, egrep, awk,
 and others.  The Crossbow scripts have not been tested under Windows,
 but a Cygwin (http://www.cygwin.com) installation with all the
 appropriate tools is probably a prerequisite.  Crossbow has been
 tested on Linux and on Mac OS X versions 10.5 and 10.6.
 
 32-bit and 64-bit Linux versions of the 'bowtie' aligner and 'soapsnp'
 SNP caller are provided in the 'bin' subdirectory.  These binaries are
 sufficient to run Crossbow on EC2.  To run Crossbow on a local, non-
 Linux Hadoop cluster, the user must be able to compile new Bowtie and
 SOAPsnp binaries for the cluster's OS.
 
 Installing Crossbow
 ===================
 
 Download the Crossbow archive from:

  http://bowtie-bio.sf.net/crossbow
 
 Extract it to the desired destination using an 'unzip' tool, then
 permanently add the expanded 'local' and 'ec2-local' subdirectories to
 your PATH.

 Checklist for Preparing to Run on Amazon Web Services
 =====================================================

 Follow each of these steps carefully before running Crossbow on
 Amazon Web Services (AWS).  Many of these steps must be performed on
 the computer from which the Crossbow will be run.
 
 Amazon's "getting started" instructions, which cover many of the same
 steps, are available here:
 
 http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/
 
 Checklist:
 ----------
 
 1. On the computer from which Crossbow will be run: Install Java, if
    not already installed.  Sun's free version of Java is available for
    download here:

    http://java.sun.com/javase/index.jsp

 2. On the computer from which Crossbow will be run: Set the JAVA_HOME
    variable to the directory that contains the "bin" subdirectory that
    in turn contains the "java" (Linux/Unix/Mac OS X) or "java.exe"
    (Windows) executable.

 3. On any computer: Create an AWS account:

    http://aws.amazon.com/
    
    Click on Sign Up Now in the upper right-hand corner and follow the
    instructions.  You will be asked to accept the AWS Customer
    Agreement.

 4. On any computer: Sign up for EC2/S3:

    http://aws.amazon.com/ec2/
    
    Click on "Sign Up For Amazon EC2" and follow the instructions.
    This step requires you to enter your credit card information.  Once
    the step is complete, your AWS account will be permitted to use
    both the Elastic Compute Cloud (EC2) service, and the Simple
    Storage Service (S3), both of which are necessary to run Crossbow.
 
 5. On the computer from which Crossbow will be run: Make note of your
    AWS credentials and download your certificate and private key.
    
    Navigate your web browser to the AWS page (http://aws.amazon.com)
    and move your mouse over "Your Account" in the upper right-hand
    corner then click "Security Credentials".  Read through the page
    that appears and make a note of the following items: (a) your
    "Access Key ID", (b) your "Secret Access Key" - you may need to
    click a link labeled "show" before it is displayed, and (c) your
    "AWS Account ID".
    
    Also, look for a section of the page that refers to your "X.509
    Certificate".  Click the "Create a New Certificate" link.  When
    prompted to download your "Private Key File" and "X.509
    Certificate", download both without altering the suggested
    filenames.  They ought to have names like:
    
      cert-HKZYKTAIG2ECMXYIBH3HXV4ZBZQ55CLO.pem
      pk-HKZYKTAIG2ECMXYIBH3HXV4ZBZQ55CLO.pem
    
    Make note of the names and where they're saved.  We recommend that
    they be saved in a directory named $HOME/.ec2.  We also recommended
    that you protect these files by removing all group and world
    permissions.  E.g.:
    
      chmod 600 $HOME/.ec2/*.pem
    
 6. On the computer from which Crossbow will be run: Permanently set
    the following environment variables.  Replace Xs and paths with the
    information noted in the previous step. 

    export AWS_ACCOUNT_ID=XXXX-XXXX-XXXX
    export AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXXXXXX
    export AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    export EC2_PRIVATE_KEY=/fill/in/path/to/pk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.pem
    export EC2_CERT=/fill/in/path/to/cert-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.pem
    
    Make sure that these variables are set correctly in your
    environment before proceeding.

 7. On the computer from which Crossbow will be run: Download and
    extract Amazon's EC2 developer tools:

    http://developer.amazonwebservices.com/connect/entry.jspa?externalID=351
    
    Permanently set the following environment variables after inserting
    the correct path and version number for the ec2-api-tools package
    into the first command:
    
    export EC2_HOME=path/to/ec2-api-tools-X.X-XXXXX
    export PATH=$PATH:$EC2_HOME/bin

 8. On the computer from which Crossbow will be run: Create and
    download an EC2 keypair.  This keypair will allow you (and the
    Crossbow scripts) to log into rented EC2 cluster computers without
    having to enter a password.  Here we assume the name given to the
    keypair is named "gsg-keypair":
    
    Type this command:
    
    $ ec2-add-keypair gsg-keypair
    
    The output should look like this:
    
    KEYPAIR	gsg-keypair	XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX
    
    -----BEGIN RSA PRIVATE KEY-----
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    (...)
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    -----END RSA PRIVATE KEY-----
    
    Copy and paste everything from "-----BEGIN RSA PRIVATE KEY-----"
    through "-----END RSA PRIVATE KEY-----" into a text file.  We
    recommend that you save it to $HOME/.ec2/id_rsa-gsg-keypair.  We
    also recommended that you protect these files by removing all group
    and world permissions.  E.g.:
    
      chmod 600 $HOME/.ec2/id_rsa-gsg-keypair
    
 9. Optional: On any computer: Request an increase to your instance
    limit.  By default, Amazon allows you to allocate EC2 clusters with
    up to 20 nodes total.  To work with more nodes, fill out the form
    on this page:
    
    http://aws.amazon.com/contact-us/ec2-request/
    
    You may have to speak to an Amazon representative and/or wait
    several business days before your request is granted.

 Building a Reference Jar
 ========================

  From Scratch
  ------------
 
  Crossbow requires three types of information about the reference
  genome:
 
  (1) The reference sequences as FASTA files, one per sequence.
  (2) A Bowtie index of the reference sequences.
  (3) A description of known SNPs and allele frequences in each
      reference sequence.
 
  The reference jar is organized as a 'sequences' subdirectory
  containing all the FASTA files (1), an 'index' subdirectory
  containing all the index files (2), and a 'snps' subdirectory
  containing all of the SNP description files (3).
 
  The FASTA files in the 'sequences' subdirectory must each be named
  chrX.fa, where X is the 0-based numeric id of the chromosome or
  sequence in the file.  For example, for a human reference, chromosome
  1's FASTA file could be named chr0.fa, chromosome 2 named chr1.fa,
  etc, all the way up to chromosomes 22, X and Y, named chr21.fa,
  chr22.fa and chr23.fa.  Also, the names of the sequences within the
  FASTA files must match the number in the file name.  I.e., the first
  line of the FASTA file chr0.fa must be ">0". 
 
  The index files in the 'index' subdirectory must have the basename
  "index".  I.e., the index subdirectory must contain these files:
 
   index.1.ebwt
   index.2.ebwt
   index.3.ebwt
   index.4.ebwt
   index.rev.1.ebwt
   index.rev.2.ebwt
 
  The index must be built using the 'bowtie-build' tool distributed
  with Bowtie.  When bowtie-build is executed, the FASTA files
  specified on the command line must be listed in ascending order of
  numeric id.  For instance, for a set of FASTA files encoding human
  chromosomes 1,2,...,22,X,Y as chr0.fa,chr1.fa,...,chr21.fa,chr22.fa,
  chr23.fa, the command for bowtie-build must list the FASTA files in
  that order:
 
   bowtie-build chr0.fa,chr1.fa,...,chr23.fa index
  
  Be sure to use bowtie-build version 0.9.8 or newer.
 
  The SNP description files in the 'snps' subdirectory must also have
  names that match the corresponding FASTA files in the 'sequences'
  subdirectory, but with extension '.snps'.  E.g. if the sequence file
  for human Chromosome 1 is named chr0.fa, then the SNP description
  file for Chromosome 1 must be named chr0.snps.  SNP description files
  may be omitted for some or all chromosomes.
 
  The format of the SNP description files must match the format
  expected by SOAPsnp's -s option.  The format consists of 1 SNP per
  line, with the following tab-separated fields per SNP:
 
   1)  Chromosome ID
   2)  1-based offset into chromosome
   3)  Whether SNP has allele frequency information (1 = yes, 0 = no)
   4)  Whether SNP is validated by experiment (1 = yes, 0 = no)
   5)  Whether SNP is actually an indel (1 = yes, 0 = no)
   6)  Frequency of A allele, as a decimal number
   7)  Frequency of C allele, as a decimal number
   8)  Frequency of G allele, as a decimal number
   9)  Frequency of T allele, as a decimal number
   10) SNP id (e.g. a dbSNP id such as "rs9976767")
 
  Once these three subdirectories have been created and populated, they
  can be combined into a single .jar file with this command:

  jar cf ref-XXX.jar sequences snps index
 
  To use ref-XXX.jar with Crossbow, you must copy it to a location
  where it can be downloaded over the internet via HTTP, FTP, or S3.
  Once it is placed in such a location, make a note if its URL.
 
  Using Automatic Scripts
  -----------------------
  
  The 'reftools' subdirectory of the Crossbow package contains scripts
  that assist in building reference jars, including scripts that handle
  the entire process of building reference jars for hg18 (UCSC human
  genome build 18) and mm9 (UCSC mouse genome build 9).  The 'db2ssnp'
  script combines SNP and allele frequency from dbSNP to create a
  chrX.snps file for the 'snps' subdirectory of the reference jar.  The
  'db2ssnp_*' scripts drive the 'db2ssnp' script for each chromosome in
  the hg18 and mm9 genomes.  The '*_jar' scripts drive the entire
  reference-jar building process, including downloading reference FASTA
  files, building a Bowtie index, and using db2ssnp to generate the
  '.snp' files, for hg18 and mm9.

 Proprocessing and Copying Reads to S3
 =====================================

 The 'cb-copy-interactive' script distributed with Crossbow is a
 convenient way to preprocess and copy reads into S3.  For finer
 control over this process, see the documentation for the
 'cb-copy-local' script or the 'ec2-master/copy_mapper.pl' script.

 Setting up and Running an Experiment on AWS
 ===========================================

 Before running an experiment using Amazon EC2, you must first (1) copy
 and preprocess the short read input data into S3 and make a note of
 the S3 directory where they are located, and (2) obtain the URL for a
 reference .jar corresponding to the organism being resequenced.  If no
 such reference.jar is yet available, you may make one yourself and
 upload it to a location that is accessible to the Hadoop cluster via
 FTP, HTTP, or S3.  For more information on (1), see previous section
 "Proprocessing and Copying Reads to S3".  For more information on (2),
 see previous section "Building a Reference Jar".
 
 The 'cb-interactive' script distributed with Crossbow is a convenient
 way to run a Crossbow job.  The script steps you through the process
 of specifying the job parameters and performs some basic sanity
 checks.  Once all the input parameters have been entered and checked,
 the script prints out a command that can be used in the future to
 start the job directly and without prompts.  For finer control of the
 Crossbow job, see the documentation for the 'cb-local' script.
 
  Monitoring your EC2 experiment
  ------------------------------
  
  Once your EC2 cluster has been allocated and your experiment is
  underway, there are a few different ways that you can monitor its
  progress.
  
   AWS Management Console
   ----------------------
  
   A simple way to monitor your EC2 activity is via the AWS Management
   Console web site:
  
    http://aws.amazon.com/console/
    (Use "Sign in to the AWS Console" button)
  
   Once you log in and navigate to the "Amazon EC2" tab, you will see a
   summary of all EC2 activity occurring under your account.

   The management console gives an updated snapshot of basic
   information regarding all your running EC2 nodes, including how many
   nodes you have allocated, what type of nodes they are, to what job
   they're allocated, etc.
  
   Click the "Running Instances" link under "My Resources" to see a list
   of all nodes allocated to your jobs.  After clicking on an instance,
   a list of details about that instance will appear on the bottom of
   the page.  Among those details is the public DNS name of the
   instance.
   
   Hadoop Job Tracker
   ------------------
   
   The Job Tracker is a good way to monitor the process of a Hadoop job
   as it is running on your EC2 cluster.  It can be used to monitor any
   kind of Hadoop job, including a Crossbow copy/preprocessing job
   (initiated via 'cb-copy-local' or 'cb-copy-interactive') or a
   Crossbow genotyping job (initiated via 'cb-local' or
   'cb-interactive').  To use the job tracker, obtain the public DNS
   name of the master node for your EC2 cluster via the AWS Management
   Console.  Then, using a web browser, navigate to the address:
   
    http://<master-DNS-name>:50030/
   
   You should see a page that says "Hadoop Map/Reduce Administration"
   at the top.  This page presents a summary of all jobs that have run
   or are running on the cluster.
   
   Unless you have established a proxy connection between your local
   computer and the EC2 master node, you will be unable to navigate to
   pages that are hosted by the other ("worker") nodes in the cluster.
   This generally prevents you from seeing information specific to a
   particular task that was run on a worker.
   
   Hadoop Job Tracker with Proxy
   -----------------------------
   
   To access Job Tracker information, including information hosted by
   the worker nodes, you must first establish a proxy connection with
   the cluster's master node.  To do so, type:
   
     cb-proxy <cluster-name>
   
   The output should look like:
   
     Proxying to host ec2-X-Y-Z-W.compute-1.amazonaws.com via local port 6666
     Gangia:     http://ec2-X-Y-Z-W.compute-1.amazonaws.com/ganglia
     JobTracker: http://ec2-X-Y-Z-W.compute-1.amazonaws.com:50030/
     NameNode:   http://ec2-X-Y-Z-W.compute-1.amazonaws.com:50070/
   
   Control will not return to the shell; this command must be left
   running for as long as the tunnel should remain open.
   
   Once the tunnel is established, you must configure your web browser
   to use the proxy service now running on localhost:6666.  For
   information on how to set this up using Firefox and FoxyProxy, see
   this document:
   
     http://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/index.html?AccessingtheHadoopUserInterfacetoMonitorJobStatus.html
 
   Make sure that your browser is set to send DNS requests through via
   the proxy.  Otherwise, worker-node DNS names cannot be resolved.
 
   Once the tunnel is established and the web browser is configured to
   use the proxy, simply navigate back to the job tracker page and all
   pages should now be accessible, including pages hosted from the
   worker nodes.
  
  Terminating your EC2 experiment
  -------------------------------
  
  You will incur a per-node-per-hour charge for however long any of the
  nodes in your cluster are still running.  Therefore, it is critical
  that you terminate your EC2 cluster as soon as you no longer need it.
  The 'cb-local' and 'cb-copy-local' scripts will automaticaly prompt
  you to terminate the cluster when the Hadoop experiment has finished
  (regardless of whether it finished successfully).  You may also
  terminate your cluster at any time by running:
  
    cb-terminate <cluster-name>
  
 Running a Crossbow job locally
 ==============================
 
 Crossbow also comes with a driver script that simplifies the task of
 running Crossbow on a local Hadoop cluster.  For details, see the
 documentation for the 'local/crossbow.pl' script.

 Crossbow Output
 ===============
 
 Once a Crossbow job completes successfully, the output is a tar
 archive named output.tar that contains a set of files named
 part-XXXXX.gz, where XXXXX is the id of the reducer that generated
 that subset of the output.  To extract the part-XXXXX.gz files, type
 'tar xfv output.tar'.  Contents of the part-XXXXX.gz files are not in
 sorted order overall, although SNPs falling within the same reference
 partition are in sorted order.  (Note that the output of the Crossbow
 job when run with the 'local/crossbow.pl' script is different.  See
 documentation for the 'local/crossbow.pl' script.)
 
 Each individual record is in the SOAPsnp output format.  SOAPsnp's
 format consists of 1 SNP per line with several tab-separated fields
 per SNP.  The fields are:
 
  1)  Chromosome ID
  2)  1-based offset into chromosome
  3)  Reference genotype
  4)  Subject genotype
  5)  Quality score of subject genotype
  6)  Best base
  7)  Average quality score of best base
  8)  Count of uniquely aligned reads corroborating the best base
  9)  Count of all aligned reads corroborating the best base
  10) Second best base
  11) Average quality score of second best base
  12) Count of uniquely aligned reads corroborating second best base
  13) Count of all aligned reads corroborating second best base
  14) Overall sequencing depth of the site
  15) Rank sum test P-value
  16) Average copy number of nearby region
  17) Whether the site is a known SNP from the file specified with -s

 For further details, see the SOAPsnp manual.
